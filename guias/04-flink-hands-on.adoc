= üåä Procesamiento de Streams con Flink e Integraci√≥n con Tableflow (45 minutos)
Luis Chavez <lachavez.olaya@gmail.com>, 2025
2025-09-11
:revdate: 2025-09-11
:linkattrs:
:ast: &ast;
:y: &#10003;
:n: &#10008;
:y: icon:check-sign[role="green"]
:n: icon:check-minus[role="red"]
:c: icon:file-text-alt[role="blue"]
:toc: auto
:toc-placement: auto
:toc-position: auto
:toc-title: Contenido de la Gu√≠a de Flink
:toclevels: 3
:idprefix:
:idseparator: -
:sectanchors:
:icons: font
:source-highlighter: highlight.js
:highlightjs-theme: idea
:experimental:

Esta gu√≠a demuestra el procesamiento de streams en tiempo real usando Apache Flink para crear tablas derivadas que ser√°n expuestas autom√°ticamente a trav√©s de Tableflow para su an√°lisis.

toc::[]

== üéØ Objetivos de Aprendizaje

Al final de esta secci√≥n, habr√°s:

* ‚úÖ Creado y configurado un pool de c√≥mputo de Flink.
* ‚úÖ Registrado t√≥picos de Kafka como tablas de Flink.
* ‚úÖ Transformado datos anidados de criptomonedas a un formato "explotado".
* ‚úÖ Escrito consultas SQL para el an√°lisis de criptomonedas en tiempo real.
* ‚úÖ Creado tablas derivadas (`price-alerts`, `crypto-trends`) para Tableflow.
* ‚úÖ Materializado las salidas de Flink como tablas Iceberg a trav√©s de Tableflow.
* ‚úÖ Consultado las tablas creadas por Flink a trav√©s de DuckDB.

== ‚è±Ô∏è Distribuci√≥n del Tiempo

* **Configuraci√≥n del Entorno Flink**: 10 minutos
* **Transformaci√≥n de Datos (Explosi√≥n)**: 10 minutos
* **Procesamiento de Streams con SQL**: 15 minutos
* **Integraci√≥n con Tableflow**: 10 minutos
* **An√°lisis con DuckDB en Tablas de Flink**: 10 minutos

== üèóÔ∏è Configuraci√≥n del Entorno Flink (10 minutos)

=== Crear Pool de C√≥mputo de Flink

[source,bash]
----
# Crear un pool de c√≥mputo para el procesamiento con Flink
confluent flink compute-pool create workshop-pool \
  --cloud aws \
  --region us-east-1 \
  --max-cfu 5 \
  --environment $CC_ENV_ID

# Listar los pools de c√≥mputo para verificar la creaci√≥n
confluent flink compute-pool list

# Usar el pool de c√≥mputo (reemplaza POOL_ID con el ID real)
# export FLINK_POOL_ID=<tu-id-de-pool>
confluent flink compute-pool use $FLINK_POOL_ID
----

=== Configurar los Ajustes del Pool de C√≥mputo

[source,bash]
----
# Describir el pool de c√≥mputo
confluent flink compute-pool describe $FLINK_POOL_ID

# Actualizar el pool de c√≥mputo si es necesario (opcional)
confluent flink compute-pool update $FLINK_POOL_ID --max-cfu 10
----

=== Configurar el Contexto del CLI de Flink

[source,bash]
----
# Verificar la conectividad de Flink
confluent flink shell --compute-pool $FLINK_POOL_ID
----

=== Explorar Esquemas y Metadatos de Tablas

[source,sql]
----
-- Mostrar todas las tablas
SHOW TABLES;

-- Describir la tabla crypto_prices
DESCRIBE `crypto-prices`;

-- Mostrar la sentencia de creaci√≥n de la tabla
SHOW CREATE TABLE `crypto-prices`;
----

== üîÑ Transformaci√≥n de Datos: Explotando Datos Anidados de Criptomonedas (10 minutos)

=== Crear Tabla de Criptomonedas Explotada

El primer paso en nuestro pipeline de proc:\Proyectos\streaming-workshop\guides\04-flink-hands-on.adoccesamiento de streams es transformar los datos anidados de criptomonedas de la API de CoinGecko en registros individuales para cada moneda.
Esta operaci√≥n de "explosi√≥n" facilita el trabajo con los datos en an√°lisis posteriores.

[source,sql]
----
include::../scripts/flink/crypto-prices-exploded.flink.sql[]
----

=== Verificar la Estructura de los Datos Explotados

[source,sql]
----
-- Comprobar la estructura de la tabla explotada
DESCRIBE `crypto-prices-exploded`;

-- Ver datos de muestra de la tabla explotada
SELECT * FROM `crypto-prices-exploded` LIMIT 10;

-- Contar registros por criptomoneda
SELECT 
    coin_id,
    COUNT(*) as record_count,
    AVG(usd) as avg_price,
    MIN(event_time) as earliest_update,
    MAX(event_time) as latest_update
FROM `crypto-prices-exploded`
GROUP BY coin_id;
----

=== Beneficios de la Explosi√≥n de Datos

El formato explotado ofrece varias ventajas:

* **An√°lisis Simplificado**: Cada fila representa una √∫nica criptomoneda, facilitando las agregaciones.
* **Mejor Rendimiento**: Las consultas pueden filtrar por `coin_id` de manera m√°s eficiente.
* **Joins m√°s Limpios**: Es m√°s f√°cil unir con otras tablas o fuentes de datos externas.
* **Esquema Estandarizado**: Todas las criptomonedas siguen la misma estructura de columnas.

== üíπ Procesamiento de Streams con SQL (15 minutos)

=== Sentencias SELECT B√°sicas en Streams de Criptomonedas Explotados

[source,sql]
----
-- Ver precios de criptomonedas en vivo desde los datos explotados
SELECT 
  coin_id,
  usd as current_price,
  usd_24h_change as daily_change_pct,
  usd_market_cap as market_cap,
  event_time
FROM `crypto-prices-exploded`
ORDER BY event_time DESC
LIMIT 20;

-- Filtrar por cambios de precio significativos usando datos explotados
SELECT 
  coin_id,
  usd as price,
  usd_24h_change as change_pct,
  usd_market_cap as market_cap,
  event_time
FROM `crypto-prices-exploded`
WHERE ABS(usd_24h_change) > 3.0;

-- Comparar precios actuales entre criptomonedas
SELECT 
  coin_id,
  usd as current_price,
  usd_24h_change as daily_change,
  CASE 
    WHEN usd_24h_change > 0 THEN 'üìà ALZA'
    WHEN usd_24h_change < 0 THEN 'üìâ BAJA'
    ELSE '‚û°Ô∏è PLANO'
  END as trend_indicator
FROM `crypto-prices-exploded`
WHERE event_time >= CURRENT_TIMESTAMP - INTERVAL '5' MINUTES;
----

=== Filtrado y Transformaciones de Cambios de Precio

[source,sql]
----
include::../scripts/flink/price-alerts.flink.sql[]
----

=== Operaciones de Ventana para Medias M√≥viles

[source,sql]
----
-- Calcular medias m√≥viles de 5 minutos usando datos explotados
SELECT 
  coin_id as cryptocurrency,
  window_start,
  window_end,
  AVG(usd) as avg_price,
  MIN(usd) as min_price,
  MAX(usd) as max_price,
  AVG(usd_market_cap) as avg_market_cap,
  AVG(usd_24h_vol) as avg_volume,
  COUNT(*) as price_updates
FROM TABLE(
  TUMBLE(TABLE `crypto-prices-exploded`, DESCRIPTOR(event_time), INTERVAL '5' MINUTES)
)
GROUP BY coin_id, window_start, window_end;

-- C√°lculo de la volatilidad del precio usando ventanas deslizantes con sintaxis TVF
SELECT 
  coin_id as cryptocurrency,
  w.window_start,
  w.window_end,
  AVG(usd) as avg_price,
  STDDEV(usd) as price_volatility,
  (MAX(usd) - MIN(usd)) / AVG(usd) * 100 as price_range_pct,
  AVG(ABS(usd_24h_change)) as avg_daily_volatility
FROM TABLE(
  HOP(TABLE `crypto-prices-exploded`, DESCRIPTOR(event_time), INTERVAL '1' MINUTES, INTERVAL '5' MINUTES)
) AS w
GROUP BY coin_id, w.window_start, w.window_end;
----

=== Crear Streams Derivados para Tendencias

[source,sql]
----
include::../scripts/flink/crypto-trends.flink.sql[]
----

== üóÑÔ∏è Integraci√≥n con Tableflow (10 minutos)

=== Materializar Tablas de Salida de Flink

Ahora que Flink est√° creando tablas derivadas, expong√°moslas a trav√©s de Tableflow para su an√°lisis:

[source,bash]
----
# Cargar variables de entorno
cd ./scripts/kafka
source .env

# Habilitar Tableflow para el t√≥pico price-alerts creado por Flink
confluent tableflow topic enable price-alerts \
  --cluster $CC_KAFKA_CLUSTER \
  --storage-type MANAGED \
  --table-formats ICEBERG \
  --retention-ms 604800000

# Habilitar Tableflow para el t√≥pico crypto-trends creado por Flink
confluent tableflow topic enable crypto-trends \
  --cluster $CC_KAFKA_CLUSTER \
  --storage-type MANAGED \
  --table-formats ICEBERG \
  --retention-ms 604800000

# Monitorear el progreso de la materializaci√≥n
confluent tableflow topic list --cluster $CC_KAFKA_CLUSTER
confluent tableflow topic describe price-alerts --cluster $CC_KAFKA_CLUSTER
confluent tableflow topic describe crypto-trends --cluster $CC_KAFKA_CLUSTER
----

=== Verificar la Materializaci√≥n de Tablas de Flink

[source,bash]
----
# Comprobar que todas las tablas est√°n ahora disponibles
confluent tableflow topic list --cluster $CC_KAFKA_CLUSTER

# Verificar que los esquemas coinciden con la salida de Flink
confluent tableflow topic describe price-alerts --cluster $CC_KAFKA_CLUSTER -o json
confluent tableflow topic describe crypto-trends --cluster $CC_KAFKA_CLUSTER -o json

# Esperar a que fluyan los datos (2-3 minutos)
confluent tableflow topic describe price-alerts --cluster $CC_KAFKA_CLUSTER -o json
----

== ü¶Ü An√°lisis con DuckDB en Tablas de Flink (10 minutos)

=== Consultar Tablas Creadas por Flink

Regresa a tu sesi√≥n de DuckDB (o inicia una nueva con `duckdb --ui workshop_analytics.db`):

[source,sql]
----
-- Consultar alertas de precios creadas por Flink
SELECT * FROM iceberg_catalog."$CC_KAFKA_CLUSTER"."price-alerts" 
ORDER BY alert_time DESC 
LIMIT 10;

-- Analizar patrones de alerta
SELECT 
    cryptocurrency,
    alert_type,
    COUNT(*) as alert_count,
    AVG(price_change) as avg_change,
    MIN(alert_time) as first_alert,
    MAX(alert_time) as latest_alert
FROM iceberg_catalog."$CC_KAFKA_CLUSTER"."price-alerts"
WHERE alert_time >= NOW() - INTERVAL 2 HOURS
GROUP BY cryptocurrency, alert_type
ORDER BY alert_count DESC;
----

=== Monitorear el Estado y las M√©tricas de la Aplicaci√≥n

[source,bash]
----
# Listar sentencias/aplicaciones de Flink en ejecuci√≥n
confluent flink statement list

# Describir una sentencia espec√≠fica
confluent flink statement describe $FLINK_STATEMENT_ID

# Comprobar los logs de la aplicaci√≥n
confluent flink statement describe $FLINK_STATEMENT_ID --show-logs
----

Tambi√©n puedes monitorear tus sentencias de Flink directamente en VS Code usando la extensi√≥n de Confluent:

.Sentencias de Flink en VS Code
image::../images/flink-statements-vscode.png[Sentencias de Flink en VS Code,800,align="center"]

=== Gestionar el Ciclo de Vida de la Aplicaci√≥n

[source,bash]
----
# Detener una sentencia en ejecuci√≥n
confluent flink statement stop $FLINK_STATEMENT_ID

# Reanudar una sentencia detenida
confluent flink statement resume $FLINK_STATEMENT_ID

# Eliminar una sentencia
confluent flink statement delete $FLINK_STATEMENT_ID
----

=== Solucionar Problemas Comunes

[source,bash]
----
# Comprobar el estado del pool de c√≥mputo
confluent flink compute-pool describe $FLINK_POOL_ID

# Ver el plan de ejecuci√≥n de la sentencia
confluent flink statement explain $FLINK_STATEMENT_ID

# Monitorear el uso de recursos
confluent flink compute-pool describe $FLINK_POOL_ID --show-metrics
----

== ‚úÖ Lista de Verificaci√≥n

Antes de pasar a la siguiente secci√≥n, aseg√∫rate de que:

- [ ] El pool de c√≥mputo de Flink est√° creado y en ejecuci√≥n.
- [ ] Los t√≥picos de Kafka est√°n registrados como tablas de Flink.
- [ ] Las consultas SQL b√°sicas se ejecutan con √©xito.
- [ ] La l√≥gica de alertas de precios est√° implementada y en ejecuci√≥n.
- [ ] Las operaciones de ventana para medias m√≥viles funcionan.
- [ ] Los t√≥picos de salida de Flink se materializan a trav√©s de Tableflow.
- [ ] Las consultas anal√≠ticas de DuckDB sobre las tablas creadas por Flink funcionan.
- [ ] El an√°lisis cruzado entre tablas de alertas y tendencias es funcional.

== üîß Entregables Clave

Al final de esta secci√≥n, deber√≠as tener:

* Un **pool de c√≥mputo de Flink** configurado y en ejecuci√≥n con recursos adecuados.
* **T√≥picos de Kafka** registrados como tablas de Flink con el esquema correcto.
* **M√∫ltiples trabajos de procesamiento de streams** ejecutando an√°lisis de criptomonedas en tiempo real.
* **Tablas creadas por Flink** materializadas como tablas Iceberg a trav√©s de Tableflow.
* **An√°lisis con DuckDB** tanto en datos de criptomonedas crudos como procesados.

== üö® Soluci√≥n de Problemas

=== Problemas del Pool de C√≥mputo

**CFUs insuficientes**::
[source,bash]
----
# Aumentar la capacidad del pool de c√≥mputo
confluent flink compute-pool update $FLINK_POOL_ID --max-cfu 10
----

**Tiempos de espera de conexi√≥n (Timeouts)**::
[source,bash]
----
# Comprobar el estado del pool de c√≥mputo
confluent flink compute-pool describe $FLINK_POOL_ID

# Reiniciar el pool de c√≥mputo si es necesario
confluent flink compute-pool stop $FLINK_POOL_ID
confluent flink compute-pool start $FLINK_POOL_ID
----

=== Problemas de Ejecuci√≥n de SQL

**Errores de "Tabla no encontrada"**::
[source,sql]
----
-- Verificar que la tabla existe
SHOW TABLES;

-- Comprobar la definici√≥n de la tabla
DESCRIBE crypto_prices;
----

**Errores de serializaci√≥n**::
- Verificar que el formato JSON coincide con el esquema de la tabla.
- Comprobar la configuraci√≥n del conector de Kafka.
- Validar los tipos de datos en la sentencia CREATE TABLE.

=== Problemas de Rendimiento

**Latencia alta**::
- Aumentar los CFUs del pool de c√≥mputo.
- Optimizar los tama√±os de las ventanas.
- Revisar la configuraci√≥n de las marcas de agua (watermarks).

**Problemas de memoria**::
- Reducir los tama√±os de las ventanas.
- Implementar el filtrado de datos en una etapa m√°s temprana del pipeline.
- Considerar estrategias de particionamiento de datos.

== üìö Recursos Adicionales

* https://docs.confluent.io/cloud/current/flink/[Confluent Cloud para Apache Flink]
* https://docs.confluent.io/confluent-cli/current/command-reference/flink/[Referencia del CLI de Flink]
* https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/table/sql/queries/window-tvf/[Funciones de Ventana de Flink SQL]

---

**Siguiente**: Procede a link:05-teardown-resources.adoc[] para la finalizaci√≥n del taller y la limpieza de recursos.
