# ğŸ“š Fundamentos de DataOps
**Bloque 1: 15 minutos**

## ğŸ¯ Â¿QuÃ© es DataOps?

**DataOps** es una metodologÃ­a que aplica principios de DevOps al desarrollo y operaciÃ³n de pipelines de datos, enfocÃ¡ndose en:

- **AutomatizaciÃ³n** de procesos de datos
- **ColaboraciÃ³n** entre equipos de datos
- **Monitoreo continuo** de calidad de datos
- **Entrega rÃ¡pida** y confiable de insights

## ğŸ”„ DataOps vs DevOps vs MLOps

| Aspecto | DevOps | DataOps | MLOps |
|---------|--------|---------|-------|
| **Foco** | Aplicaciones | Pipelines de datos | Modelos ML |
| **Artefacto** | CÃ³digo | Datos + CÃ³digo | Modelos + Datos |
| **Calidad** | Tests unitarios | Tests de datos | ValidaciÃ³n de modelos |
| **Monitoreo** | Uptime, latencia | Calidad, frescura | Drift, performance |

## ğŸ—ï¸ Principios Fundamentales

### 1. **AutomatizaciÃ³n First**
```bash
# Manual (âŒ)
confluent kafka topic create crypto-prices
confluent connect cluster create --config-file config.json

# Automatizado (âœ…)
./setup-pipeline.sh
```

### 2. **Testing de Datos**
```python
# ValidaciÃ³n automÃ¡tica
def test_data_freshness(data):
    age = current_time - data['last_updated_at']
    assert age < 300  # < 5 minutos
```

### 3. **Observabilidad**
```python
# MÃ©tricas en tiempo real
metrics = {
    'throughput': messages_per_second,
    'error_rate': errors / total_messages,
    'data_quality_score': passed_tests / total_tests
}
```

### 4. **ColaboraciÃ³n**
- **Data Engineers:** Construyen pipelines
- **Data Scientists:** Definen calidad
- **DevOps:** Proveen infraestructura
- **Business:** Definen SLAs

## ğŸ¯ Beneficios en Nuestro Proyecto

### Antes (Sin DataOps)
- âŒ Setup manual de 30+ minutos
- âŒ Problemas de datos detectados tarde
- âŒ Sin visibilidad del pipeline
- âŒ Deployments manuales propensos a errores

### DespuÃ©s (Con DataOps)
- âœ… Setup automatizado en 5 minutos
- âœ… DetecciÃ³n proactiva de problemas
- âœ… Monitoreo en tiempo real
- âœ… CI/CD automatizado

## ğŸ“Š Casos de Uso Reales

### 1. **Netflix**
- Pipelines de recomendaciones con DataOps
- Tests automÃ¡ticos de calidad de datos
- Monitoreo de drift en tiempo real

### 2. **Uber**
- ValidaciÃ³n de datos de viajes
- Alertas automÃ¡ticas por anomalÃ­as
- Pipeline de ML con DataOps

### 3. **Airbnb**
- Tests de integridad de datos de reservas
- Monitoreo de mÃ©tricas de negocio
- AutomatizaciÃ³n de reportes

## ğŸ› ï¸ Herramientas del Ecosistema DataOps

### Testing
- **Great Expectations:** Tests de calidad de datos
- **Deequ:** ValidaciÃ³n de datos en Spark
- **Custom Tests:** Como nuestro `data-quality-tests.py`

### Monitoreo
- **Datadog:** MÃ©tricas de pipelines
- **Prometheus + Grafana:** Observabilidad
- **Custom Monitoring:** Como nuestro `pipeline-monitor.py`

### OrquestaciÃ³n
- **Apache Airflow:** Workflows de datos
- **Prefect:** OrquestaciÃ³n moderna
- **GitHub Actions:** CI/CD para datos

## ğŸ¯ AplicaciÃ³n en Nuestro Workshop

Vamos a implementar DataOps en nuestro pipeline de criptomonedas:

```mermaid
graph LR
    A[CoinGecko API] --> B[Kafka Connect]
    B --> C[Kafka Topics]
    C --> D[Flink Processing]
    D --> E[Tableflow]
    E --> F[DuckDB]
    
    G[DataOps Layer] --> B
    G --> C
    G --> D
    G --> E
    
    G --> H[Automation]
    G --> I[Testing]
    G --> J[Monitoring]
    G --> K[CI/CD]
```

### Componentes que Implementaremos:
1. **AutomatizaciÃ³n:** `setup-pipeline.sh`
2. **Testing:** `data-quality-tests.py`
3. **Monitoreo:** `pipeline-monitor.py`
4. **CI/CD:** GitHub Actions workflow

## ğŸ’¡ Preguntas de ReflexiÃ³n

1. **Â¿QuÃ© problemas de datos has enfrentado en proyectos anteriores?**
2. **Â¿CÃ³mo detectarÃ­as actualmente un problema en un pipeline de streaming?**
3. **Â¿QuÃ© mÃ©tricas consideras mÃ¡s importantes para un pipeline de criptomonedas?**

---

**Siguiente:** [DiseÃ±o de Pipeline DataOps](02-diseÃ±o-pipeline.md)